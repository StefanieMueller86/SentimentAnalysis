{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff8debc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTM\n",
    "by Stefanie Müller\n",
    "\n",
    "Data Source: https://www.kaggle.com/kazanova/sentiment140\n",
    "\n",
    "Data description:\n",
    "1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "2. ids: The id of the tweet ( 2087)\n",
    "3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. user: the user that tweeted (robotickilldozr)\n",
    "6. text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07917b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2505812d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Sentiment          ID                          Date     Query  \\\n0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n...            ...         ...                           ...       ...   \n1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n\n                    User                                               Text  \n0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1          scotthamilton  is upset that he can't update his Facebook by ...  \n2               mattycus  @Kenichan I dived many times for the ball. Man...  \n3                ElleCTF    my whole body feels itchy and like its on fire   \n4                 Karoli  @nationwideclass no, it's not behaving at all....  \n...                  ...                                                ...  \n1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n\n[1600000 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentiment</th>\n      <th>ID</th>\n      <th>Date</th>\n      <th>Query</th>\n      <th>User</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>4</td>\n      <td>2193601966</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>AmandaMarie1028</td>\n      <td>Just woke up. Having no school is the best fee...</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>4</td>\n      <td>2193601969</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>TheWDBoards</td>\n      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>4</td>\n      <td>2193601991</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>bpbabe</td>\n      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>4</td>\n      <td>2193602064</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>tinydiamondz</td>\n      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>4</td>\n      <td>2193602129</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>RyanTrevMorris</td>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", sep=',', encoding=\"ISO-8859-1\", header=None) \n",
    "data.columns = [\"Sentiment\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478de873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Sentiment     int64\nID            int64\nDate         object\nQuery        object\nUser         object\nText         object\ndtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0254ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    800000\n4    800000\nName: Sentiment, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking sentiments\n",
    "data[\"Sentiment\"].value_counts() #there are no neutral reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e818bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([800000, 800000], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Specifying x and y\n",
    "x = data.iloc[:,5].to_numpy()\n",
    "y = data.iloc[:,0].to_numpy() / 4 #Normalizing sentiments\n",
    "\n",
    "print(np.unique(y, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c261ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([640000, 640000], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Train-Test-Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)\n",
    "\n",
    "print(np.unique(y_train, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ac1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying values\n",
    "max_length = 25\n",
    "sequence_length = 80\n",
    "vocab_size = 50000\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3600efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer removes all punctuations\n",
    "tokenizer = keras.preprocessing.text.Tokenizer( \n",
    "    num_words=vocab_size,\n",
    "    filters='\"!#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', #blacklist\n",
    "    lower=True, split=' ', \n",
    "    char_level=False, \n",
    "    oov_token=0,\n",
    "    document_count=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470f69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating internal vocabulary\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991337d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594368\n"
     ]
    }
   ],
   "source": [
    "#Amount of different words\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9ce7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming texts into sequences of integers\n",
    "encoded_docs_xtrain = tokenizer.texts_to_sequences(x_train)\n",
    "encoded_docs_xtest = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "padded_xtrain = keras.preprocessing.sequence.pad_sequences(encoded_docs_xtrain,\n",
    "                                                           maxlen=max_length, padding='post')\n",
    "padded_xtest = keras.preprocessing.sequence.pad_sequences(encoded_docs_xtest,\n",
    "                                                          maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6889a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Models\n",
    "\n",
    "def dense_model(model):\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def lstm_model(model):\n",
    "    model.add(keras.layers.LSTM(100,\n",
    "                                activation='tanh',\n",
    "                                dropout=0.1,\n",
    "                                recurrent_dropout=0.1,\n",
    "                                #input_shape=(sequence_length, 1),\n",
    "                                return_sequences=True)) #must be True, if lstm follows\n",
    "\n",
    "    model.add(keras.layers.LSTM(100, activation='tanh')) #no GlobalAveragePooling needed with this layer since it reduces the dimensions from 3 to 2\n",
    "    model.add(keras.layers.Dense(7, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def prepare_model(type):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "\n",
    "    if type == 'dense':\n",
    "        model = dense_model(model)\n",
    "    elif type == 'lstm':\n",
    "        model = lstm_model(model)\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fit_model(model, epochs=10, batch_size=4000, verbose=True):\n",
    "    history = model.fit(padded_xtrain,\n",
    "                        y_train,\n",
    "                        #class_weight = class_weights,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        validation_data=(padded_xtest, y_test))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GPU...\") if tf.config.list_physical_devices('GPU') else print(\"Training on CPU...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 16)            800000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 800,181\n",
      "Trainable params: 800,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.5964 - accuracy: 0.7146 - val_loss: 0.4926 - val_accuracy: 0.7810\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.4624 - accuracy: 0.7957 - val_loss: 0.4532 - val_accuracy: 0.7980\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.4377 - accuracy: 0.8062 - val_loss: 0.4410 - val_accuracy: 0.8021\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.4250 - accuracy: 0.8103 - val_loss: 0.4343 - val_accuracy: 0.8033\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.4166 - accuracy: 0.8131 - val_loss: 0.4302 - val_accuracy: 0.8040\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.4105 - accuracy: 0.8149 - val_loss: 0.4278 - val_accuracy: 0.8044\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.4059 - accuracy: 0.8164 - val_loss: 0.4266 - val_accuracy: 0.8047\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.4021 - accuracy: 0.8175 - val_loss: 0.4256 - val_accuracy: 0.8046\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.3988 - accuracy: 0.8187 - val_loss: 0.4255 - val_accuracy: 0.8048\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.3960 - accuracy: 0.8197 - val_loss: 0.4256 - val_accuracy: 0.8048\n"
     ]
    }
   ],
   "source": [
    "#Dense model preparation and fit\n",
    "model_dense = prepare_model('dense')\n",
    "history_dense = fit_model(model_dense, epochs=10, batch_size=4000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f38153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 16)            800000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25, 100)           46800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 707       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 927,915\n",
      "Trainable params: 927,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1280/1280 [==============================] - 129s 99ms/step - loss: 0.4439 - accuracy: 0.7901 - val_loss: 0.4128 - val_accuracy: 0.8097\n"
     ]
    }
   ],
   "source": [
    "#LSTM model preparation and fit\n",
    "model_lstm = prepare_model('lstm')\n",
    "history_lstm = fit_model(model_lstm, epochs=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "facb78be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804756224155426\n"
     ]
    }
   ],
   "source": [
    "#Dense accuracy\n",
    "_, accuracy_dense = model_dense.evaluate(padded_xtest, y_test, verbose=0)\n",
    "print(accuracy_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24698946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8097187280654907\n"
     ]
    }
   ],
   "source": [
    "#LSTM accuracy\n",
    "_, accuracy_lstm = model_lstm.evaluate(padded_xtest, y_test, verbose=0)\n",
    "print(accuracy_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparison with others on kaggle shows that the data seems to give no better score than around 80% accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single Sentences Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45471301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dense: [[0.94381714]] vs. LSTM: [[0.94938016]]\n"
     ]
    }
   ],
   "source": [
    "#Positive Sentiment correctly identified:\n",
    "x_pred = [\"Best tweet ever!\"]\n",
    "encoded_docs_pred = tokenizer.texts_to_sequences(x_pred)\n",
    "\n",
    "padded_docs_pred = keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_docs_pred,\n",
    "    maxlen=max_length,\n",
    "    padding='post')\n",
    "\n",
    "y_pred_dense = model_dense.predict(\n",
    "    padded_docs_pred,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "\n",
    "y_pred_lstm = model_lstm.predict(\n",
    "    padded_docs_pred,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "\n",
    "print(f'Accuracy Dense: {y_pred_dense} vs. LSTM: {y_pred_lstm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dense: [[0.10881341]] vs. LSTM: [[0.05100768]]\n"
     ]
    }
   ],
   "source": [
    "#Negative Sentiment correctly identified:\n",
    "x_pred = [\"Worst tweet ever!\"]\n",
    "encoded_docs_pred = tokenizer.texts_to_sequences(x_pred)\n",
    "\n",
    "padded_docs_pred = keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_docs_pred,\n",
    "    maxlen=max_length,\n",
    "    padding='post')\n",
    "\n",
    "y_pred_dense = model_dense.predict(\n",
    "    padded_docs_pred,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "\n",
    "y_pred_lstm = model_lstm.predict(\n",
    "    padded_docs_pred,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "\n",
    "print(f'Accuracy Dense: {y_pred_dense} vs. LSTM: {y_pred_lstm}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Dense: [[0.83951706]] vs. LSTM: [[0.7635558]]\n"
     ]
    }
   ],
   "source": [
    "#Here the sentiment analysis does not work out. This is because of the word \"least\" is classified as a more positive than negative sentiment.\n",
    "x_pred = [\"My least favorite.\"]\n",
    "encoded_docs_pred = tokenizer.texts_to_sequences(x_pred)\n",
    "\n",
    "padded_docs_pred = keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_docs_pred,\n",
    "    maxlen=max_length,\n",
    "    padding='post')\n",
    "\n",
    "y_pred_dense = model_dense.predict(\n",
    "    padded_docs_pred,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "\n",
    "y_pred_lstm = model_lstm.predict(\n",
    "    padded_docs_pred,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "\n",
    "print(f'Accuracy Dense: {y_pred_dense} vs. LSTM: {y_pred_lstm}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see the LSTM-Net performs a little better which makes sense based on the fact that LSTM uses long/short-term memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}